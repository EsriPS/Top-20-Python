{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c026995",
   "metadata": {},
   "source": [
    "## Tutorial 08-01 - Attribute Manipulation with DataFrames\n",
    "\n",
    "Now we’re ready to dig in and get our hands dirty cleaning up some data.  Let’s go back to our work with GeoNinjas PythonAnalytics.  Today, let’s say we’ve been given a job to summarize some public works data for the city of San Francisco.  We’ve been given a CSV file of individual calls to the city via their “311” app.  We’ve been tasked with evaluating the data, cleaning it, and summarizing the calls by neighborhood.  Ultimately, we’d like to know the count of calls in each neighborhood and how long it takes to resolve them on average.  Let’s start by reading the data and doing some data exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eaa89ce",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Read and Explore Data from a CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f24d996",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 1. Read the 311 data from a CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e607aa",
   "metadata": {
    "hidden": true
   },
   "source": [
    "One of the wonderful things about pandas is that it can read from (and write to) a wide variety of formats and save you tons of time writing code.  In this case, we’re going to be reading data from a CSV file.  It’s as simple as running the following line of code with the path to your local CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039e8721",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "df = pandas.read_csv(\"../Chapter 05 - Jupyter Notebooks/311_cases.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445648b8",
   "metadata": {
    "hidden": true
   },
   "source": [
    "It’s as simple as that.  You’ve read all that data into memory and now you have a DataFrame that you can start working with.  So now that we’ve got that data, you’ll need to start understanding what you’re looking at.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af6a77d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 2. Explore the 311 data using pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cebb57f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Pandas contains a suite of handy tools for getting familiar with data.  You can start with a couple quick methods and properties that will help you understand what the data in a DataFrame looks like at a glance.  First, you'll see how big the DataFrame is by running the following property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b0367c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f09873",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The shape property of a DataFrame returns some really basic but really useful information.  It returns a tuple describing the shape of the data.  The first value in the tuple is the number of rows in the data.  The second is the number of columns in the data.  In my case, calling the shape property returned (47540, 48) which means that the DataFrame is 47,540 rows long and 48 columns wide. \n",
    "\n",
    "It seems that 48 columns is a lot of information.  For the purposes of our analysis and summary, it’s unlikely that you need all those columns, but let’s take a look and see what they are.  You can use another handy property of the DataFrame to understand what columns there are and what kinds of data might be in each of them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d82aa68",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e2548b",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Remove Unnecessary Columns and Rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9eb1ff6",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 1.  Drop any unnecessary columns from the DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3df71a5",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Glancing at the resulting column descriptions from this property, you might notice some things that give you an idea of what this data is all about.  What will probably jump right out at you is that there are several columns that start with the word **DELETE**.  It’d probably be a good idea if you dropped those right off the bat.  First, you can use the `.columns` property of the DataFrame to identify all those columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa48d88",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "drop_cols = [c for c in df.columns if \"DELETE\" in c]\n",
    "drop_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c5339d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This will return a list containing just the columns that we want to get rid of (or drop).  Now to remove those columns, you can use the DataFrame's `drop()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd02915",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = df.drop(columns = drop_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393113e0",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 2. Explore the quality of latitude and longitude data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c6af06",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Notice in the line above that you actually redefined your DataFrame.  This is common with pandas.  Operations like **drop** will actually return another DataFrame.  You could either capture that output with a new variable or overwrite our existing variable (like you did in this case).\n",
    "\n",
    "Now that you’ve trimmed down our DataFrame to exclude some irrelevant columns, let’s take a look and see if there are any rows we want to exclude from our analysis.  You may have already noticed that there are columns for Latitude and Longitude.  Since we’re largely interested in geospatial data, this is great.  At this point, though, the skeptical analysts among us might still be a little dubious about the quality of this data.  This is totally justified, as we’ll see.  Let’s have a look at the distribution of our Latitude data and see if there are any major red flags.  \n",
    "\n",
    "You can use the `.describe()` method on a pandas.Series object to see some statistics about the distribution of the values in that column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12410693",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['Latitude'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40178a34",
   "metadata": {
    "hidden": true
   },
   "source": [
    "What you got from that `describe()` method is a series of values that give us an idea of the contents and distribution of this numeric field.  You can see from the count that there are 47,540 values.  You can also see the mean and standard distribution.  Where this gets interesting is when we look at the min value.  You can see that there are values with no latitude in our data.  This is not great for geospatial analysis.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460f41f7",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 3.  Exclude any records with no latitude and longitude values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84193de",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let’s exclude those records from our analysis going forward.  First you’ll have to identify the records, which is where we can explore some additional pandas syntax.  Similarly to how we used a square bracket to identify a single column, you can also use square brackets to provide a true/false condition to constrain the records in a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f73c356",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df[df['Latitude'] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91f5884",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this line of code, you’ve written some criteria that test each record.  If the Latitude value in a record is 0, it will return True.  Otherwise, it returns False.  Those criteria inform the DataFrame which records you want to return.  In this case, you’ve returned the 9,183 records that have no Latitude.  These are the records you want to exclude going forward.  So you can modify our criteria to return records that have a non-zero Latitude and overwrite our DataFrame to only save those records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275b3efc",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = df[df['Latitude'] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e3c9cc",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now you’ve got a DataFrame that has valid Latitude values.  If we call the DataFrame’s shape property now, we’ll see that there are 38, 357 rows and 37 columns.  As an extra step, you can take a look at the Longitude column now.  If you call the `describe()` method on the Longitude column, you should see that there are no zero values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75541a1",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Create and Modify Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8ddb87",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 1.  Convert a string column to a date column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522f9734",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Before you get started with your summary of the data, you should ensure that our analysis columns are the right data type.  In this case, we’re going to want to know the difference between the time each case was “Opened” and “Closed”.  When you looked at the DataFrame’s dtypes property earlier, though, you may have noticed that these columns were “objects” (which is pandas’ catchall string data type)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf796e9",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# showing \"Opened\" as a string/object type\n",
    "df['Opened']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0e9dcd",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To get the difference between two date columns, you first need to ensure that all the data in those columns is actually datetime data.  You can do that with a pandas conversion.  \n",
    "\n",
    "Here, you'll use the `to_datetime()` method to convert a string Series (or column) to a date Series.  The `.to_datetime()` method accepts an argument where you can specify the format of your input date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f05e1e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pandas.to_datetime(df['Opened'], \n",
    "                   format=\"%m/%d/%Y %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddd0b9c",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 2.  Overwrite existing columns in the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad6d2af",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The `to_datetime()` method has returned a pandas Series (pandas terminology for a column) that has the dtype that you’re looking for.  Now that all this data is converted to datetime, you can actually do some analysis.  You'll save the conversion results for the Opened column and also convert the Closed column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf597b7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['Opened'] = pandas.to_datetime(df['Opened'], \n",
    "                                  format=\"%m/%d/%Y %H:%M:%S\")\n",
    "df['Closed'] = pandas.to_datetime(df['Closed'], \n",
    "                                  format=\"%m/%d/%Y %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c125aea5",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In the code above, you’ve taken the results of the `to_datetime()` method and overwritten the values that were in the two columns you’re interested in.  Now if you were to look at the dtypes property of the DataFrame, you’d see that the dtype for these columns has changed from “object” to “datetime64[ns]”.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bc7a16",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 3.  Get the time difference between two date columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc217ee",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now that you’ve converted our Opened and Closed columns to datetime values, you can subtract the Opened time from the Closed time to find the duration of time between when a call was received to when the case was resolved.  This can tell you how long it’s taking to deal with each of these cases.  This is super easy to do with pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09efc7a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['Closed'] - df['Opened']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bac6a87",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This returns a new Series that contains *timedelta* data.  This is a pandas data type that shows the difference between two datetimes.  This is one of the metrics you’ve been asked to report on, so you can save it as a new column in our DataFrame.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6318a675",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['OpenTime'] = df['Closed'] - df['Opened']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05af49e5",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Creating a new column in a DataFrame is surprisingly easy.  You just define it with the same square-bracket syntax we used to access an existing column.  Now when you print or return the DataFrame, the data in the previous screenshot will be saved as a new column called “OpenTime”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21bab90",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Summarize and Save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13855238",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 1.  Summarize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42be7557",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now that the data is cleaned and converted to data you can use, you can summarize data by neighborhood.  You’ll use a pandas method called `groupby()` to do this.  You’ll also to specify which statistic types we want to use for your summary and use a method called `.agg()` to return them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2582292c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_neighborhood = df.groupby(\"Neighborhood\").agg(\n",
    "    {\n",
    "        \"OpenTime\": \"mean\",\n",
    "        \"CaseID\": \"count\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bf6493",
   "metadata": {
    "hidden": true
   },
   "source": [
    "You may notice in the code snippet above that we’re providing a dictionary to the `.agg()` method.  This method accepts arguments in a number of different formats.  It’s worth checking out the pandas documentation, which is quite good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0de75bd",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 2. Write the summary to a file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d27c127",
   "metadata": {
    "hidden": true
   },
   "source": [
    "From here, you’re going to need to create an output.  We’ll get into creating geospatial data in the second exercise in our chapter, but in this case, you can just write our data to a CSV file.  This way you can allow our stakeholders to review the product of your analysis before you go any further.  Pandas makes writing to file just as easy as reading from a file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713a0e06",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_neighborhood.to_csv(\"./311_neighborhood.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
